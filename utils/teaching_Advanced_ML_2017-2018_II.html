<!DOCTYPE html>
<html lang="en">
	<head>
			<meta name="author" content="Zoltan Szabo" />
			<meta name="description" content="Zoltán Szabó's Home Page (Department of Statistics, LSE)" />
			<meta name="keywords" content="information theory, kernel methods, statistical machine learning, scalable computation,
			randomized algorithms, shape-constrained prediction, hypothesis testing, safety-critical learning, style transfer,
			distribution regression, dictionary learning, structured sparsity, independent subspace analysis, bioinformatics,
			Bayesian inference, computer vision, finance, economics, analysis of climate data, criminal data analysis, collaborative filtering,
			emotion recognition, face tracking, remote sensing, natural language processing, gene analysis" />
			<meta charset="UTF-8">
	   	<title> Zoltán Szabó's Home Page </title>
   	   <style>
   	    		@import url("css/style.css");
    		</style>
	</head>
	<body>
			   <div class="center menu">
	  	 			<nav>
   	    			<a href="../index.html">Home</a> -
  	   	 			<a href="../publications.html">Publications</a> -
     	 				<a href="../service.html">Service</a> -
     	 				<a href="../talks.html">Talks</a> -
							<a href="../software.html">Software</a> -
              <a href="../teaching.html">Teaching</a>
   				</nav>
  				</div>

			  	<div class="left">
					<p class="name"> <b>Teaching</b>: 2018, Spring (École Polytechnique):
		      </div>

				<div class="left">
					Advanced Machine Learning (MSc Big Data For Business - X/HEC): MAP541 (HEC561)
					<ul>
						 <li>Goal:
							 <ul>
									 <li> Machine learning is a scientific discipline that is concerned with the design and development of algorithms that allow computers to learn from data. A major focus of machine learning is to automatically learn complex patterns and to make intelligent decisions based on them. The set of possible data inputs that feed a learning task can be very large and diverse, which makes modelling and prior assumptions critical problems for the design of relevant algorithms.
										 		This course focuses on the methodology underlying supervised and unsupervised learning, with a particular emphasis on the mathematical formulation of algorithms, and the way they can be implemented and used in practice. We will therefore describe some necessary tools from optimization theory, and explain how to use them for machine learning. Numerical illustrations will be given for most of the studied methods.
                   </li>
									 <li> We will follow the book from Hastie, Tibshirani and Friedman entitled "Elements of Statistical Learning". This will define the structure of the course even if we will sometimes complement the book during the lectures.
									 </li>
							 </ul>
						 </li>

           <li>Lecturers:
						 <ul>
							 <li> <a href="http://www.cmap.polytechnique.fr/~lepennec/en/">Erwan Le Pennec</a>, <a href="http://asi.insa-rouen.fr/enseignants/~scanu/">Stéphane Canu</a>,
							 Zoltán Szabó, <a href="http://www.cmap.polytechnique.fr/~anne.auger/">Anne Auger</a>.
						 </li>
						 </ul>
					 </li>

					 <li>
						 	Teaching material, exercises, lab tasks:
              <ul>
								<li> Available in Moodle. </li>
              </ul>
           </li>

				</ul>
							<hr>
				<ul>

					    <li>Lecture 1 (Jan. 8):
  					    		<ul>
 	    				 					<li> Overview of Supervised Learning. </li>
                 		</ul>
              </li>

							<li>Lecture 2 (Jan. 15):
  					    		<ul>
 	    				 					<li>  Linear Methods for Regression. </li>
                 		</ul>
              </li>

							<li>Lecture 3 (Jan. 29):
  					    		<ul>
 	    				 					<li> Linear Methods for Classification. </li>
                 		</ul>
              </li>

							<li>Lecture 4 (Feb. 5):
  					    		<ul>
 	    				 					<li> Model Assessment and Selection. </li>
                 		</ul>
              </li>

							<li>Lecture 5 (Feb. 12):
  					    		<ul>
 	    				 					<li> Trees,  Boosting. </li>
                 		</ul>
              </li>

							<li>Lecture 6 (Feb. 19):
  					    		<ul>
 	    				 					<li> Averaging, Random Forests, Ensemble Methods. </li>
                 		</ul>
              </li>

							<li>Lecture 7 (Mar. 5):
  					    		<ul>
 	    				 					<li> Neural Networks. </li>
                 		</ul>
              </li>

							<li>Lecture 8 (Mar. 12):
  					    		<ul>
 	    				 					<li> Support Vector Machines. </li>
                 		</ul>
              </li>
						</ul>
									<hr>
						<ul>

							<li>References:
  					    		<ul>
 	    				 					<li> [1] Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie. <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">The Elements of Statistical Learning - Data Mining, Inference</a>. Springer, 2008. </li>
 	    				 					<li> [2] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. <a href="http://www.springer.com/fr/book/9781461471370">An Introduction to Statistical Learning with Applications in R</a>. Springer (8th printing), 2017. </li>
												<li> [3] David Donoho. <a href="http://www.tandfonline.com/doi/abs/10.1080/10618600.2017.1384734">50 Years of Data Science</a>. Journal of Computational and Graphical Statistics, 26(4): 745-766, 2017.  </li>
												<li> [4] Robert Tibshirani. <a href="https://statweb.stanford.edu/~tibs/lasso/lasso.pdf">Regression Shrinkage and Selection via the Lasso</a>. Journal of the Royal Statistical Society, 58(1): 267-288, 1996.  </li>
                 		</ul>
              </li>
  	  				</ul>
  				</div>

	</body>
</html>
